import os
import json
from pathlib import Path
from tqdm import tqdm
from datasets.arrow_writer import ArrowWriter


def load_token_data(token_file_path):
    """
    åŠ è½½tokenæ–‡ä»¶ï¼Œè¿”å›ä¸€ä¸ªkey->codeçš„å­—å…¸
    """
    token_dict = {}
    if not os.path.exists(token_file_path):
        raise FileNotFoundError(f"Tokenæ–‡ä»¶ä¸å­˜åœ¨: {token_file_path}")

    print(f"åŠ è½½tokenæ–‡ä»¶: {token_file_path}")
    with open(token_file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(tqdm(f, desc="åŠ è½½token"), 1):
            line = line.strip()
            if not line:
                continue
            try:
                token_entry = json.loads(line)
                if "key" in token_entry and "code" in token_entry:
                    token_dict[token_entry["key"]] = token_entry["code"]
                else:
                    print(f"âš ï¸ ç¬¬{line_num}è¡Œç¼ºå°‘ key æˆ– code å­—æ®µ")
            except json.JSONDecodeError as e:
                print(f"âš ï¸ ç¬¬{line_num}è¡Œè§£æé”™è¯¯: {e}")
    print(f"âœ… å…±åŠ è½½ {len(token_dict)} ä¸ª token æ¡ç›®")
    return token_dict


def process_custom_jsonl(jsonl_path, token_dict):
    """
    å¤„ç†è‡ªå®šä¹‰ jsonl æ–‡ä»¶ï¼Œè¿”å›å¤„ç†ç»“æœ
    """
    print(f"\nè¯»å–æ•°æ®æ–‡ä»¶: {jsonl_path}")
    results = []
    durations = []
    missing_token_count = 0

    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(tqdm(f, desc="å¤„ç†æ¡ç›®"), 1):
            try:
                obj = json.loads(line)
                audio_id = obj["id"]
                audio_path = obj["path"]
                duration = obj["duration"]

                # åŸºæœ¬è¿‡æ»¤
                if duration < 0.4 or duration > 60:
                    continue

                audio_token = token_dict.get(audio_id, [])
                if not audio_token:
                    missing_token_count += 1
                    continue

                results.append({
                    "audio_path": audio_path,
                    "duration": duration,
                    "audio_token": audio_token
                })
                durations.append(duration)

            except Exception as e:
                print(f"âš ï¸ ç¬¬{line_num}è¡Œè§£æå¤±è´¥: {e}")

    print(f"\nâœ… æˆåŠŸå¤„ç†æ ·æœ¬æ•°: {len(results)}")
    print(f"âŒ ç¼ºå¤±tokençš„æ ·æœ¬æ•°: {missing_token_count}")
    print(f"â±ï¸ æ€»æ—¶é•¿: {sum(durations) / 3600:.2f} å°æ—¶")
    return results, durations


def save_dataset(results, durations, save_dir):
    """
    ä¿å­˜ä¸º arrow å’Œ duration.json æ–‡ä»¶
    """
    os.makedirs(save_dir, exist_ok=True)
    arrow_path = os.path.join(save_dir, "raw.arrow")
    duration_path = os.path.join(save_dir, "duration.json")

    print(f"\nğŸ’¾ å†™å…¥ arrow æ–‡ä»¶: {arrow_path}")
    with ArrowWriter(path=arrow_path) as writer:
        for entry in tqdm(results, desc="å†™å…¥ arrow"):
            writer.write(entry)

    print(f"ğŸ’¾ å†™å…¥ duration æ–‡ä»¶: {duration_path}")
    with open(duration_path, 'w', encoding='utf-8') as f:
        json.dump({"duration": durations}, f, ensure_ascii=False)


def main():
    # ==== è¾“å…¥è·¯å¾„é…ç½® ====
    jsonl_path = "data/aslp_hq_data/aslp_hq_data.jsonl"
    token_file_path = "/home/node56_tmpdata/xcli/data/ASLP_HQ_s3tokenizer_v1_50hz/merged_tokens.jsonl"
    save_dir = "data/aslp_hq_data/ASLP_HQ_s3tokenizer_v1_50hz"

    # ==== å¤„ç†æµç¨‹ ====
    token_dict = load_token_data(token_file_path)
    results, durations = process_custom_jsonl(jsonl_path, token_dict)
    save_dataset(results, durations, save_dir)


if __name__ == "__main__":
    main()
